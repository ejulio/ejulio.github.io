<!doctype html><html lang=pt-br><head><title>A normalização e o Gradient Descent · Júlio César Batista
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Júlio César Batista"><meta name=description content="Vamos falar sobre machine learning.
Estou [enfim] participando do MOOC do Andrew Ng, Ph. D. no Coursera e me deparei com a importância de normalizar os dados antes de efetuar a otimização do algoritmo de aprendizado.
Não leve a mal, sei, há certo tempo, que é importante normalizar os valores de entrada para que o algoritmo tenha uma melhor, e mais rápida, convergência.
Entretanto, nunca havia, ao menos até onde percebi, me deparado com o quanto esse pré-processamento implica no processo."><meta name=keywords content="blog,software,developer"><meta name=fediverse:creator content><meta name=twitter:card content="summary"><meta name=twitter:title content="A normalização e o Gradient Descent"><meta name=twitter:description content="Vamos falar sobre machine learning. Estou [enfim] participando do MOOC do Andrew Ng, Ph. D. no Coursera e me deparei com a importância de normalizar os dados antes de efetuar a otimização do algoritmo de aprendizado. Não leve a mal, sei, há certo tempo, que é importante normalizar os valores de entrada para que o algoritmo tenha uma melhor, e mais rápida, convergência. Entretanto, nunca havia, ao menos até onde percebi, me deparado com o quanto esse pré-processamento implica no processo."><meta property="og:url" content="https://juliocesarbatista.com/posts/a-normalizacao-e-o-gradient-descent/"><meta property="og:site_name" content="Júlio César Batista"><meta property="og:title" content="A normalização e o Gradient Descent"><meta property="og:description" content="Vamos falar sobre machine learning. Estou [enfim] participando do MOOC do Andrew Ng, Ph. D. no Coursera e me deparei com a importância de normalizar os dados antes de efetuar a otimização do algoritmo de aprendizado. Não leve a mal, sei, há certo tempo, que é importante normalizar os valores de entrada para que o algoritmo tenha uma melhor, e mais rápida, convergência. Entretanto, nunca havia, ao menos até onde percebi, me deparado com o quanto esse pré-processamento implica no processo."><meta property="og:locale" content="pt_br"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2017-09-27T00:00:00+00:00"><meta property="article:modified_time" content="2017-09-27T00:00:00+00:00"><meta property="article:tag" content="Aprendizado De Máquina"><meta property="article:tag" content="Exemplos"><meta property="article:tag" content="Python"><meta property="article:tag" content="Normalização De Caracterśiticas"><meta property="article:tag" content="Gradient Descent"><meta property="article:tag" content="Otimização"><link rel=canonical href=https://juliocesarbatista.com/posts/a-normalizacao-e-o-gradient-descent/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.07092c1350ffd254998dc43a44ae96e617d14af4df4602626878df89189c5e1a.css integrity="sha256-BwksE1D/0lSZjcQ6RK6W5hfRSvTfRgJiaHjfiRicXho=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://juliocesarbatista.com/>Júlio César Batista
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=/tags/publica%c3%a7%c3%a3o/>Publicações</a></li><li class=navigation-item><a class=navigation-link href=/tags/projeto/>Projetos</a></li><li class=navigation-item><a class=navigation-link href=/tags/notas/>Notas</a></li><li class=navigation-item><a class=navigation-link href=/tags/>Tags</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://juliocesarbatista.com/posts/a-normalizacao-e-o-gradient-descent/>A normalização e o Gradient Descent</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2017-09-27T00:00:00Z>2017-09-27
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
3 minutos de leitura</span></div><div class=tags><i class="fa-solid fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/aprendizado-de-m%C3%A1quina/>Aprendizado De Máquina</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/exemplos/>Exemplos</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/python/>Python</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/normaliza%C3%A7%C3%A3o-de-caracter%C5%9Biticas/>Normalização De Caracterśiticas</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/gradient-descent/>Gradient Descent</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/otimiza%C3%A7%C3%A3o/>Otimização</a></span></div></div></header><div class=post-content><p>Vamos falar sobre machine learning.
Estou [enfim] participando do <em><a href=https://www.coursera.org/learn/machine-learning class=external-link target=_blank rel=noopener>MOOC</a></em> do <a href=https://twitter.com/AndrewYNg class=external-link target=_blank rel=noopener>Andrew Ng, Ph. D.</a> no <a href=http://coursera.org class=external-link target=_blank rel=noopener>Coursera</a> e me deparei com a importância de normalizar os dados antes de efetuar a otimização do algoritmo de aprendizado.
Não leve a mal, sei, há certo tempo, que é importante normalizar os valores de entrada para que o algoritmo tenha uma melhor, e mais rápida, convergência.
Entretanto, nunca havia, ao menos até onde percebi, me deparado com o quanto esse pré-processamento implica no processo.</p><p>Se você já leu ou fez algum exemplo/tutorial sobre <em>machine learning</em>, provavelmente, sabe que é importante normalizar as entradas/características antes de iniciar o processo de otimização.
Geralmente, isso é feito com <em>standardizing</em> ao aplicar $\frac{\mathbf{x}_j - \mathbf{\mu}_j}{\mathbf{\sigma}_j}$ que vai deixar as características ($\mathbf{x}$) com média ($\mathbf{\mu}$) 0 e desvido padrão ($\mathbf{\sigma}$) 1.
Especialmente no caso do <em>gradient descent</em> (método de otimização baseado em derivadas), essa normalização é importante para &ldquo;garantir uma superfície mais uniforme&rdquo; de forma que o algoritmo consegue caminhar em direção ao melhor ponto da otimização.
Nesse caso, estava <a href=https://github.com/ejulio/coursera-machine-learning/blob/master/Week%203/Assignment%20Logistic%20Regression.ipynb class=external-link target=_blank rel=noopener>implementando</a> o <em>gradient descent</em> para otimizar uma simples regressão logísitica com 2 variáveis de entradas, um problema razoavelmente simples.
Porém, quando executei a rotina com ~1,000 iterações ao algoritmo mal e mal saiu do lugar.
Passei um tempo revirando o código, achei uns possíveis bugs e arrumei algumas coisas.
Mesmo assim, nada de otimizar, ou chegar próximo do valor esperado no exercício.
Com isso, fui verificar o fórum para ver se alguém teve o mesmo problema e me deparei com um usuário que precisou de 1,000,000 de iterações para convergir implementando em R.
Bem, fiz o teste e, realmente, com 1,000,000 de iterações minha implementação em python (não quero usar o MATLAB :b) convergiu para um valor aproximado ao informado no exercício.
Enfim, não fazia muito sentido 1,000,000 de iterações para um problema de classificação binária com 2 variáveis.
Depois de pesquisar mais um pouco e ver em algum lugar do fórum a palavra mágica <strong>normalização</strong>, me lembrei que não havia normalizado as características antes de iniciar a otimização.
Dito e feito, implementei a normalização e com um teste com ~10,000 iterações a minha implementação convergiu para o valor esperado no exercício.</p><p>Lição aprendida! Apesar de saber da importância de normalizar as entradas do algoritmo, nunca tinha pensado sobre o quão difícil é o problema sem normalizar.
Agora, tive um ótimo exemplo do que realmente está acontecendo durante esse processo de otimização.
Enfim, o que escrevi aqui não deve ser novidade para quem já fez algo com <em>machine learning</em>, mas pode ser que ajude a quem está começando a ter uma noção do porque de alguns detalhes serem importantes para o processo como um todo.</p></div><footer></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-vKruj+a13U8yHIkAyGgK1J3ArTLzrFGBbBc0tDp4ad/EyewESeXE/Iv67Aj8gKZ0 crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-PwRUT/YqbnEjkZO0zZxNqcxACrXe+j766U2amXcgMg5457rve2Y7I6ZJSm2A0mS4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>©
2016 -
2024
Júlio César Batista
·
Promovido por <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-1BE56W9H8M"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1BE56W9H8M")}</script></body></html>