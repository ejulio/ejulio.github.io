<!doctype html><html lang=pt-br><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=content-language content="pt-br"><meta name=color-scheme content="light dark"><meta name=author content="Júlio César Batista"><meta name=description content="Vamos falar sobre machine learning. Estou [enfim] participando do MOOC do Andrew Ng, Ph. D. no Coursera e me deparei com a importância de normalizar os dados antes de efetuar a otimização do algoritmo de aprendizado. Não leve a mal, sei, há certo tempo, que é importante normalizar os valores de entrada para que o algoritmo tenha uma melhor, e mais rápida, convergência. Entretanto, nunca havia, ao menos até onde percebi, me deparado com o quanto esse pré-processamento implica no processo."><meta name=keywords content="blog,software,developer"><meta name=twitter:card content="summary"><meta name=twitter:title content="A normalização e o Gradient Descent"><meta name=twitter:description content="Vamos falar sobre machine learning. Estou [enfim] participando do MOOC do Andrew Ng, Ph. D. no Coursera e me deparei com a importância de normalizar os dados antes de efetuar a otimização do algoritmo de aprendizado. Não leve a mal, sei, há certo tempo, que é importante normalizar os valores de entrada para que o algoritmo tenha uma melhor, e mais rápida, convergência. Entretanto, nunca havia, ao menos até onde percebi, me deparado com o quanto esse pré-processamento implica no processo."><meta property="og:title" content="A normalização e o Gradient Descent"><meta property="og:description" content="Vamos falar sobre machine learning. Estou [enfim] participando do MOOC do Andrew Ng, Ph. D. no Coursera e me deparei com a importância de normalizar os dados antes de efetuar a otimização do algoritmo de aprendizado. Não leve a mal, sei, há certo tempo, que é importante normalizar os valores de entrada para que o algoritmo tenha uma melhor, e mais rápida, convergência. Entretanto, nunca havia, ao menos até onde percebi, me deparado com o quanto esse pré-processamento implica no processo."><meta property="og:type" content="article"><meta property="og:url" content="https://juliocesarbatista.com/posts/a-normalizacao-e-o-gradient-descent/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2017-09-27T00:00:00+00:00"><meta property="article:modified_time" content="2017-09-27T00:00:00+00:00"><title>A normalização e o Gradient Descent · Júlio César Batista</title><link rel=canonical href=https://juliocesarbatista.com/posts/a-normalizacao-e-o-gradient-descent/><link rel=preload href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.d9fddbffe6f27e69985dc5fe0471cdb0e57fbf4775714bc3d847accb08f4a1f6.css integrity="sha256-2f3b/+byfmmYXcX+BHHNsOV/v0d1cUvD2Eesywj0ofY=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.002ee2378e14c7a68f1f0a53d9694ed252090987c4e768023fac694a4fc5f793.css integrity="sha256-AC7iN44Ux6aPHwpT2WlO0lIJCYfE52gCP6xpSk/F95M=" crossorigin=anonymous media=screen><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><meta name=generator content="Hugo 0.118.2"></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=/>Júlio César Batista</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/categories/publica%c3%a7%c3%a3o/>Publicações</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=/categories/>Categorias</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://juliocesarbatista.com/posts/a-normalizacao-e-o-gradient-descent/>A normalização e o Gradient Descent</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa fa-calendar" aria-hidden=true></i>
<time datetime=2017-09-27T00:00:00Z>2017-09-27</time></span>
<span class=reading-time><i class="fa fa-clock-o" aria-hidden=true></i>
3 minutos de leitura</span></div><div class=categories><i class="fa fa-folder" aria-hidden=true></i>
<a href=/categories/aprendizado-de-m%C3%A1quina/>Aprendizado de Máquina</a></div><div class=tags><i class="fa fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/exemplos/>Exemplos</a></span>
<span class=separator>•</span>
<span class=tag><a href=/tags/python/>python</a></span>
<span class=separator>•</span>
<span class=tag><a href=/tags/normaliza%C3%A7%C3%A3o-de-caracter%C5%9Biticas/>Normalização de caracterśiticas</a></span>
<span class=separator>•</span>
<span class=tag><a href=/tags/gradient-descent/>Gradient descent</a></span>
<span class=separator>•</span>
<span class=tag><a href=/tags/otimiza%C3%A7%C3%A3o/>Otimização</a></span></div></div></header><div><p>Vamos falar sobre machine learning.
Estou [enfim] participando do <em><a href=https://www.coursera.org/learn/machine-learning>MOOC</a></em> do <a href=https://twitter.com/AndrewYNg>Andrew Ng, Ph. D.</a> no <a href=http://coursera.org>Coursera</a> e me deparei com a importância de normalizar os dados antes de efetuar a otimização do algoritmo de aprendizado.
Não leve a mal, sei, há certo tempo, que é importante normalizar os valores de entrada para que o algoritmo tenha uma melhor, e mais rápida, convergência.
Entretanto, nunca havia, ao menos até onde percebi, me deparado com o quanto esse pré-processamento implica no processo.</p><p>Se você já leu ou fez algum exemplo/tutorial sobre <em>machine learning</em>, provavelmente, sabe que é importante normalizar as entradas/características antes de iniciar o processo de otimização.
Geralmente, isso é feito com <em>standardizing</em> ao aplicar $\frac{\mathbf{x}_j - \mathbf{\mu}_j}{\mathbf{\sigma}_j}$ que vai deixar as características ($\mathbf{x}$) com média ($\mathbf{\mu}$) 0 e desvido padrão ($\mathbf{\sigma}$) 1.
Especialmente no caso do <em>gradient descent</em> (método de otimização baseado em derivadas), essa normalização é importante para &ldquo;garantir uma superfície mais uniforme&rdquo; de forma que o algoritmo consegue caminhar em direção ao melhor ponto da otimização.
Nesse caso, estava <a href=https://github.com/ejulio/coursera-machine-learning/blob/master/Week%203/Assignment%20Logistic%20Regression.ipynb>implementando</a> o <em>gradient descent</em> para otimizar uma simples regressão logísitica com 2 variáveis de entradas, um problema razoavelmente simples.
Porém, quando executei a rotina com ~1,000 iterações ao algoritmo mal e mal saiu do lugar.
Passei um tempo revirando o código, achei uns possíveis bugs e arrumei algumas coisas.
Mesmo assim, nada de otimizar, ou chegar próximo do valor esperado no exercício.
Com isso, fui verificar o fórum para ver se alguém teve o mesmo problema e me deparei com um usuário que precisou de 1,000,000 de iterações para convergir implementando em R.
Bem, fiz o teste e, realmente, com 1,000,000 de iterações minha implementação em python (não quero usar o MATLAB :b) convergiu para um valor aproximado ao informado no exercício.
Enfim, não fazia muito sentido 1,000,000 de iterações para um problema de classificação binária com 2 variáveis.
Depois de pesquisar mais um pouco e ver em algum lugar do fórum a palavra mágica <strong>normalização</strong>, me lembrei que não havia normalizado as características antes de iniciar a otimização.
Dito e feito, implementei a normalização e com um teste com ~10,000 iterações a minha implementação convergiu para o valor esperado no exercício.</p><p>Lição aprendida! Apesar de saber da importância de normalizar as entradas do algoritmo, nunca tinha pensado sobre o quão difícil é o problema sem normalizar.
Agora, tive um ótimo exemplo do que realmente está acontecendo durante esse processo de otimização.
Enfim, o que escrevi aqui não deve ser novidade para quem já fez algo com <em>machine learning</em>, mas pode ser que ajude a quem está começando a ter uma noção do porque de alguns detalhes serem importantes para o processo como um todo.</p></div><footer></footer></article><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload='renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})'></script></section></div><footer class=footer><section class=container>©
2016 -
2023
Júlio César Batista
·
Promovido por <a href=https://gohugo.io/>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/>Coder</a>.</section></footer></main><script src=/js/coder.min.9cf2dbf9b6989ef8eae941ffb4231c26d1dc026bca38f1d19fdba50177d8a9ac.js integrity="sha256-nPLb+baYnvjq6UH/tCMcJtHcAmvKOPHRn9ulAXfYqaw="></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1BE56W9H8M"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1BE56W9H8M",{anonymize_ip:!1})}</script></body></html>