<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Júlio César Batista</title><link>https://juliocesarbatista.com/posts/</link><description>Recent content in Posts on Júlio César Batista</description><generator>Hugo</generator><language>pt-br</language><lastBuildDate>Wed, 21 Feb 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://juliocesarbatista.com/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Uma introdução ao Apache Kafka</title><link>https://juliocesarbatista.com/posts/uma-introducao-ao-apache-kafka/</link><pubDate>Wed, 21 Feb 2024 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/uma-introducao-ao-apache-kafka/</guid><description>&lt;p>Material da minha palestra &lt;em>Uma introdução ao Apache Kafka&lt;/em> que &lt;a href="https://www.meetup.com/hackerspaceblumenau/events/299181482/" class="external-link" target="_blank" rel="noopener">apresentei no Hackerspace Blumenau&lt;/a>.
Os slides estão em inglês porque eu fiz a mesma apresentação na &lt;a href="https://shippo.com" class="external-link" target="_blank" rel="noopener">Shippo&lt;/a>.&lt;/p>
&lt;p>&lt;a href="https://juliocesarbatista.com/palestras/uma-introducao-ao-apache-kafka.pdf" >Slides&lt;/a>&lt;/p>
&lt;p>&lt;a href="https://www.youtube.com/watch?v=JflNHhYdrzQ" class="external-link" target="_blank" rel="noopener">YouTube&lt;/a>&lt;/p></description></item><item><title>A tale of concurrency through creativity in Python: a deep dive into how gevent works</title><link>https://juliocesarbatista.com/posts/a-tale-of-concurrency-through-creativity-in-python/</link><pubDate>Tue, 28 Nov 2023 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/a-tale-of-concurrency-through-creativity-in-python/</guid><description>&lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
 &lt;iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/GunMToxbE0E?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"
 >&lt;/iframe>
 &lt;/div>

&lt;ul>
&lt;li>&lt;code>gevent&lt;/code> usa &lt;code>greenlets&lt;/code> (corotinas ou, &lt;code>goroutines&lt;/code> em &lt;em>go&lt;/em>) ao invés de &lt;em>threads&lt;/em> do sistema operacional.
&lt;ul>
&lt;li>Essas operações são colaborativas, portanto elas precisam ceder (&lt;code>yield&lt;/code>) a execução para outras rotinas.&lt;/li>
&lt;li>Por serem apenas rotinas no programa, sem intervenção do sistema operacional, são mais leves.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Interessante como a biblioteca em &lt;em>C&lt;/em> &lt;code>gevent&lt;/code> resolve o problema.
&lt;ul>
&lt;li>A biblioteca manipula a &lt;em>stack&lt;/em>/&lt;em>heap&lt;/em> para dar continuidade em uma rotina que deceu a execução.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;code>libev&lt;/code> foi usada para manipular o &lt;em>loop&lt;/em> de eventos e controlar a execução das &lt;code>greenlets&lt;/code>&lt;/li>
&lt;li>Com &lt;em>monkey patching&lt;/em> é possível tornar um código síncrono em assíncrono.
&lt;ul>
&lt;li>Possível porque &lt;code>gevent&lt;/code> troca &lt;em>socket&lt;/em> bloqueante por não bloqueante.&lt;/li>
&lt;li>Porém só funciona para biblioteca em &lt;em>python&lt;/em>.
&lt;ul>
&lt;li>Se estiver usando uma biblioteca de banco de dados em &lt;em>C&lt;/em> com &lt;em>wrapper&lt;/em> em &lt;em>python&lt;/em>, não vai funcionar.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Só funciona para &lt;em>I/O&lt;/em>, rotinas que precisam de muita CPU não fazem muito sentido rodar com &lt;em>greenlets&lt;/em> (provavelmente que &lt;em>threads&lt;/em> sejam um modelo melhor).&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Deve haver colaboração entre as rotinas, se uma delas não ceder a execução para que as demais continuem quando estiverem prontas, não haverá concorrência, apenas uma rotina continua.&lt;/li>
&lt;/ul></description></item><item><title>Índices invertidos e algoritmos de busca</title><link>https://juliocesarbatista.com/posts/indices-invertidos-e-algoritmos-de-busca/</link><pubDate>Sun, 12 Nov 2023 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/indices-invertidos-e-algoritmos-de-busca/</guid><description>&lt;p>&lt;a href="https://juliocesarbatista.com/palestras/indices-invertidos-e-algoritmos-de-busca.pdf" >Material da minha palestra sobre os fundamentos de mecanismos de busca no 5° encontro do Elastic Blumenau User Group.&lt;/a>&lt;/p></description></item><item><title>Isolamento de transações em bancos de dados relacionais</title><link>https://juliocesarbatista.com/posts/isolamento-transacoes-bancos-dados-relacionais/</link><pubDate>Thu, 09 Nov 2023 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/isolamento-transacoes-bancos-dados-relacionais/</guid><description>&lt;p>Estava lendo sobre os níveis de &lt;a href="https://www.postgresql.org/docs/current/transaction-iso.html" class="external-link" target="_blank" rel="noopener">isolamento de transações em bancos de dados relacionais, especificamente no PostgreSQL&lt;/a> e decidi fazer um projeto em python para simular os cenários de concorrência.
&lt;a href="https://github.com/ejulio/transaction-isolation-levels" class="external-link" target="_blank" rel="noopener">O projeto está disponível no GitHub&lt;/a> e espero que ajude a elucidar alguns detalhes sobre o
funcionamento de transações em bancos de dados.
Usei &lt;code>asyncio&lt;/code> para a &amp;ldquo;concorrência&amp;rdquo; e gerenciar a execução entre duas &amp;ldquo;threads&amp;rdquo;, permitindo a interpolação de comandos SQL
em duas transações separadas em momento especîficos para avaliar os resultados.&lt;/p></description></item><item><title>Trabalho remoto 101</title><link>https://juliocesarbatista.com/posts/trabalho_remoto_101/</link><pubDate>Tue, 10 Oct 2023 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/trabalho_remoto_101/</guid><description>&lt;p>&lt;a href="https://juliocesarbatista.com/palestras/trabalho-remoto-101.pdf" >Material da minha palestra sobre trabalho remoto para empresas no exterior no SEMINCO da FURB em 2023.&lt;/a>&lt;/p></description></item><item><title>Em busca de sentido</title><link>https://juliocesarbatista.com/posts/em-busca-de-sentido/</link><pubDate>Sat, 23 Sep 2023 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/em-busca-de-sentido/</guid><description>&lt;p>Recentemente terminei de ler o livro &lt;a href="https://www.amazon.com.br/Em-Busca-Sentido-psic%C3%B3logo-concentra%C3%A7%C3%A3o/dp/8532606261" class="external-link" target="_blank" rel="noopener">&lt;em>Em busca de sentido: um psicólogo no campo de concentração&lt;/em>&lt;/a> de Viktor E. Frankl.
O livro é dividido em duas partes, sendo que a primeira conta algumas observações/experiências do autor durante os anos em que passou como prisioneiro em um campo de concentração durante a Segunda Guerra Mundial.
A segunda parte do livro aborta os fundamentos da &lt;em>logoterapia&lt;/em>, uma abordagem de terapia que &lt;em>se concentra na busca de sentido como fonte motivadora&lt;/em> do ser humando, que foi desenvolvida pelo autor.&lt;/p></description></item><item><title>Grafos e consultas SQL recursivas</title><link>https://juliocesarbatista.com/posts/queries_sql_recursivas/</link><pubDate>Mon, 04 Sep 2023 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/queries_sql_recursivas/</guid><description>&lt;p>Recentemente me deparei com o problema de trabalhar com grafos com SQL.
Dado o modelo de dados abaixo, como resolver (no fim do artigo existem alguns exemplos de grafos que podem ser usados para teste):&lt;/p>
&lt;ol>
&lt;li>Detectar se existem ciclos no grafo&lt;/li>
&lt;li>Encontrar todos os caminhos no grafo&lt;/li>
&lt;/ol>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-gdscript3" data-lang="gdscript3">&lt;span style="display:flex;">&lt;span>create table graph (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>	id varchar(100) primary key,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>	name varchar(500) &lt;span style="font-weight:bold">not&lt;/span> null
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>create table node (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>	id varchar(100),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>	graph_id varchar(100) references graph(id),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>	name varchar(500) &lt;span style="font-weight:bold">not&lt;/span> null,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>	primary key (graph_id, id)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>);
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>create table edge (
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>	id varchar(100),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>	graph_id varchar(100) references graph(id),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>	from_id varchar(100),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>	to_id varchar(100),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>	cost &lt;span style="font-weight:bold">float&lt;/span> &lt;span style="font-weight:bold">not&lt;/span> null,
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>	foreign key (graph_id, from_id) references node(graph_id, id),
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>	foreign key (graph_id, to_id) references node(graph_id, id)
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>);
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Não vou entrar nos méritos no modelo, existem várias formas de modelar o grafo.
O ponto principal é como escrever um &lt;em>query&lt;/em> SQL para resolver o problema.&lt;/p></description></item><item><title>Os primeiros anos do Brasil</title><link>https://juliocesarbatista.com/posts/os-primeiros-anos-do-brasil/</link><pubDate>Fri, 04 Aug 2023 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/os-primeiros-anos-do-brasil/</guid><description>&lt;p>Recentemente terminei de ler a coleção &lt;a href="https://www.amazon.com.br/Box-Cole%C3%A7%C3%A3o-Brasilis-Descobrimento-Traficantes/dp/8556080561/" class="external-link" target="_blank" rel="noopener">Brasilis&lt;/a> de &lt;a href="https://www.youtube.com/c/BuenasIdeias" class="external-link" target="_blank" rel="noopener">Eduardo Bueno&lt;/a> que fala dos primeiros 50 anos do Brasil.
Os quatro livros são muito bons, mas o primeiro me causou mais impacto.
A primeiro livro conta a história da viagem de Pedro Álvares Cabral que chegou no Brasil, atual Porto Seguro na Bahia, em 22 de Abril de 1500.
Além dos relatos da viagem, o livro constrói a história da navegação portuguesa que culminou na travessia do Oceano Atlântico.
Particularmente, sabia que Cristóvão Colombo havia navegado para oeste para chegar a India, mas não sabia porque os portugueses &amp;ldquo;fizeram&amp;rdquo; o mesmo sendo que o caminho deles era contornando a África.
O detalhe é que a navegação para contornar a África não podia ser feito pela costa, devido aos ventos para o norte.
Portanto, os portugueses tinham que fazer uma manobra indo para dentro do Oceano Atlântico para pegar ventos para o sul e poder contornar o continente.
O primeiro navegador a fazer tal proesa foi Bartolomeu Dias que relatou a manobra que a esquadra de Pedro Álvares Cabral faria anos depois.
É importante notar que ao fazer essa manobra pelo Atlântico, eles notaram que poderiam existir terras para oeste e talvez tenha sido isso que fez com que Cabral tenha ido além ao oeste e chegado no Brasil.
Para mim, o que mais me chamou a atenção foi a longa jornada e as várias viagens que ocorreram até chegar na viagem de Cabral que chegou no Brasil.
Foram anos de desenvolvimento e aperfeiçoamento dos portugueses para chegar nesse momento.&lt;/p></description></item><item><title>Designing Data Intensive Applications - Chapter 2: Data Models and Query Languages</title><link>https://juliocesarbatista.com/posts/ddia-chapter-2/</link><pubDate>Fri, 31 Mar 2023 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/ddia-chapter-2/</guid><description>&lt;p>O modelo de dados é uma das principais partes de um sistema, porque ele influencia como pensamos sobre o problema.
Esse ponto é bem importante, principalmente se pensamos em algoritmos e estruturas de dados.
Muitas vezes usamos a abstração errada e isso torna o problema mais complexo.
Por exemplo, você tem uma lista de itens &lt;code>[a, b, c, a, a, b, c, d, d, c, f]&lt;/code> e você quer remover os itens duplicados. Como resolver esse problema?
Se pensarmos apenas na lista como uma abstração, vamos ter que escrever um pouco de código para resolver esse problema.
Porém, podemos pensar em outra abstração, como um conjunto (&lt;code>set&lt;/code>) e esse problema está resolvido.
O motivo é que um conjunto, por definição, não mantém valores duplicados.
Então o conjunto final seria algo como &lt;code>{a, b, c, d, f}&lt;/code>.
Outro exemplo é com grafos.
Podemos pensar neles como estruturas, &lt;code>Vertex&lt;/code> e &lt;code>Edge&lt;/code>, e todas as arestas de um determinado nó estão em uma lista, algo como &lt;code>vertex.edges&lt;/code>.
Entretanto, é possível representar um grafo a partir de uma matriz de adjacência que permite usar técnicas da álgebra linear para resolver alguns problemas, assim abrindo as portas para diferentes soluções e possivelmente reaproveitando técnicas de outras áreas sem precisar reinventar a roda.
Note que a ideia aqui é simplesmente explorar como as diferentes representações nos fazem pensar sobre o problema e suas soluções.&lt;/p></description></item><item><title>Sociedade do cansaço</title><link>https://juliocesarbatista.com/posts/sociedade-do-cansaco/</link><pubDate>Sun, 12 Mar 2023 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/sociedade-do-cansaco/</guid><description>&lt;p>Sociedade do cansaço, livro escrito por Byung-Chul Han.
Este é um livro curto com cerca de cento e trinta páginas e traz uma reflexão interessante sobre a sociedade atual.
Aqui vou deixar um pouco da minha interpretação do livro e como vejo isso acontecendo, principalmente na área de tecnologia.&lt;/p>
&lt;p>A princípio, o tema central do livro é que vivemos em uma sociedade cansada e que as doenças como depressão, ansiedade, burnout,
déficit de atenção e hiperatividade são os efeitos desse cansaço.
Basicamente, vivemos em uma sociedade onde tudo é possível, onde temos direito a tudo, e com esse excesso de possibilidades sentimos
a culpa de não darmos o melhor.
Este efeito se torna ainda maior quando paramos de usar como referência apenas as pessoas no nosso círculo mais próximo e expandimos
isso ao nível do mundo.
Com o advento do trabalho remoto nos dias atuais, você pode estar trabalhando com alguém que passou pelas melhores empresas e
instituições de ensino pelo mundo e sempre ter o sentimento de estar atrás em algum aspecto, dessa forma, se força a
correr atrás da diferença.
Não que isso seja ruim, querer melhorar é algo positivo, mas pode ser um problema quando isso toma conta da sua vida,
principalmente ao se sobrecarregar e não conseguir mais parar até eliminar a diferença.
Porém, a diferença pode nunca ser eliminada, visto que ao mesmo tempo que você melhora, as outras pessoas também melhoram e
assim acabamos presos em um ciclo sem fim.
A solução é então sempre nos compararmos a nós mesmos, tornando essa comparação justa e tangível.&lt;/p></description></item><item><title>Diagram layout engines: Minimizing hierarchical edge crossings</title><link>https://juliocesarbatista.com/posts/diagram-layout-engines-edge-crossings/</link><pubDate>Thu, 26 Jan 2023 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/diagram-layout-engines-edge-crossings/</guid><description>&lt;p>No blog da Terrastruct &lt;a href="https://terrastruct.com/blog/post/diagram-layout-engines-crossing-minimization/" class="external-link" target="_blank" rel="noopener">https://terrastruct.com/blog/post/diagram-layout-engines-crossing-minimization/&lt;/a>&lt;/p></description></item><item><title>Designing Data Intensive Applications - Chapter 1: Reliable, Scalable, and Maintainable Applications</title><link>https://juliocesarbatista.com/posts/ddia-chapter-1/</link><pubDate>Mon, 16 Jan 2023 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/ddia-chapter-1/</guid><description>&lt;p>Um sistema de dados intensivos (&lt;em>data intensive applications&lt;/em>) é definido por ter os dados
como o principal desafio, seja a quantidade, complexidade, ou velocidade com que eles mudam.&lt;/p>
&lt;p>As três características principais são: confiança (&lt;em>reliability&lt;/em>),
escalabilidade (_scalability) e Manutenibilidade (&lt;em>maintainability&lt;/em>).&lt;/p>
&lt;h1 id="confiança-_reliability_">
 Confiança (&lt;em>Reliability&lt;/em>)
 &lt;a class="heading-link" href="#confian%c3%a7a-_reliability_">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link para o cabeçalho">&lt;/i>
 &lt;span class="sr-only">Link para o cabeçalho&lt;/span>
 &lt;/a>
&lt;/h1>
&lt;p>Basicamente, o sistema deve funcionar como o esperado.
O desempenho é bom para a carga normal do sistema;
mesmo que um usuário faça um erro, o sistema continua operando normalmente;
apenas usuários autorizados podem usar a aplicação;
assim por diante.&lt;/p></description></item><item><title>Designing Data Intensive Applications</title><link>https://juliocesarbatista.com/posts/ddia/</link><pubDate>Sun, 15 Jan 2023 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/ddia/</guid><description>&lt;p>Estou lendo &lt;em>Designing Data Intensive Applications&lt;/em> de Martin Kleppman.
Conforme for avançando na leitura, vou manter por aqui algumas anotações sobre o livro.
Não vou fazer questão de escrever como um artigo ou algo similar, apenas um conjunto de anotações/ideias/reflexões.
Se quiser, &lt;a href="https://juliocesarbatista.com/tags/designing-data-intensive-applications/" >pode acompanhar aqui&lt;/a>.&lt;/p></description></item><item><title>How to reduce noise in the data through data parsing</title><link>https://juliocesarbatista.com/posts/how-to-reduce-noise-in-the-data-through-data-parsing/</link><pubDate>Tue, 31 Aug 2021 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/how-to-reduce-noise-in-the-data-through-data-parsing/</guid><description>&lt;p>No blog da Zyte &lt;a href="https://www.zyte.com/blog/how-to-reduce-noise-in-the-data-through-data-parsing/" class="external-link" target="_blank" rel="noopener">https://www.zyte.com/blog/how-to-reduce-noise-in-the-data-through-data-parsing/&lt;/a>&lt;/p></description></item><item><title>Scrapy Cloud secrets: Hub Crawl Frontier and how to use it</title><link>https://juliocesarbatista.com/posts/scrapy-cloud-secrets-hub-crawl-frontier-and-how-to-use-it/</link><pubDate>Thu, 06 Aug 2020 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/scrapy-cloud-secrets-hub-crawl-frontier-and-how-to-use-it/</guid><description>&lt;p>No blog da Zyte &lt;a href="https://www.zyte.com/blog/scrapy-cloud-secrets-hub-crawl-frontier-and-how-to-use-it/" class="external-link" target="_blank" rel="noopener">https://www.zyte.com/blog/scrapy-cloud-secrets-hub-crawl-frontier-and-how-to-use-it/&lt;/a>&lt;/p></description></item><item><title>Custom crawling &amp; News API: designing a web scraping solution</title><link>https://juliocesarbatista.com/posts/extract-articles-at-scale-designing-a-web-scraping-solution/</link><pubDate>Tue, 28 Apr 2020 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/extract-articles-at-scale-designing-a-web-scraping-solution/</guid><description>&lt;p>No blog da Zyte &lt;a href="https://www.zyte.com/blog/extract-articles-at-scale-designing-a-web-scraping-solution/" class="external-link" target="_blank" rel="noopener">https://www.zyte.com/blog/extract-articles-at-scale-designing-a-web-scraping-solution/&lt;/a>&lt;/p></description></item><item><title>Provas matemáticas diretas</title><link>https://juliocesarbatista.com/posts/provas-matematicas-diretas/</link><pubDate>Mon, 23 Mar 2020 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/provas-matematicas-diretas/</guid><description>&lt;p>Na matemática, provas de argumentos são obtidas a partir de um processo de raciocínio lógico.
Por mais criativo que possa ser esse processo, existem algumas &amp;ldquo;fórmulas&amp;rdquo; que podem ajudar no caminho.
O primeiro passo é formular o argumento que requer uma prova.
Isso é possível ao escrever uma proposição (afirmação) que só pode ter dois resultados possíveis: &lt;em>verdadeiro&lt;/em> ou &lt;em>falso&lt;/em>, nunca os dois juntos.
Assim, é possível construir uma argumentação para chegar na conclusão se determinada proposição é verdadeira ou não.
Seguem alguns exemplos de proposições:&lt;/p></description></item><item><title>Correção ortográfica com índice k-gram</title><link>https://juliocesarbatista.com/posts/correcao-ortografica-k-gram-index/</link><pubDate>Tue, 11 Feb 2020 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/correcao-ortografica-k-gram-index/</guid><description>&lt;p>Ao escrever uma consulta, o usuário pode cometer erros ortográficos durante a digitação.
Esses erros podem ter duas formas: escrita incorreta da palavra (&lt;em>comesso&lt;/em> ao invés de &lt;em>começo&lt;/em>), contexto (&lt;em>no meu casa&lt;/em> ao invés de &lt;em>na minha casa&lt;/em>).
Note que no primeiro exemplo, a palavra está incorreta; no segundo, as palavras estão corretas mas o contexto é errado.
Nesse momento, a ideia é ver como é possível fazer a correção de erros de escrito (primeiro caso).
Uma forma de fazer isso é comparando partes menores das palavras (&lt;em>substrings&lt;/em>, ou &lt;em>k-grams&lt;/em>).
Um &lt;em>k-gram&lt;/em> define uma &lt;em>substring&lt;/em> de tamanho &lt;em>k&lt;/em>.
Portanto, os &lt;em>3-grams&lt;/em> de &lt;em>começo&lt;/em> são: &lt;code>com&lt;/code>, &lt;code>ome&lt;/code>, &lt;code>meç&lt;/code>, &lt;code>eço&lt;/code>.
Um detalhe importante é que, é comum adicionar $k - 1$ caractéres especiais, normalmente &lt;code>$&lt;/code> no início e fim da palavra.
Assim, os &lt;em>3-grams&lt;/em> de &lt;em>começo&lt;/em> são: &lt;code>$$c&lt;/code>, &lt;code>$co&lt;/code>, &lt;code>com&lt;/code>, &lt;code>ome&lt;/code>, &lt;code>meç&lt;/code>, &lt;code>eço&lt;/code>, &lt;code>ço$&lt;/code>, &lt;code>o$$&lt;/code>.
Uma vez que os &lt;em>3-grams&lt;/em> da palavra são conhecidos, é possível efetuar o mesmo procedimento para a palavras que será usada na comparação.
Nesse caso, &lt;em>comesso&lt;/em>: &lt;code>$$c&lt;/code>, &lt;code>$co&lt;/code>, &lt;code>com&lt;/code>, &lt;code>ome&lt;/code>, &lt;code>mes&lt;/code>, &lt;code>ess&lt;/code>, &lt;code>sso&lt;/code>, &lt;code>so$&lt;/code>, &lt;code>o$$&lt;/code>.&lt;/p></description></item><item><title>Executando consultas por frases: Positional Index</title><link>https://juliocesarbatista.com/posts/phrase-queries-positional-index/</link><pubDate>Mon, 20 Jan 2020 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/phrase-queries-positional-index/</guid><description>&lt;p>Para realizar a consulta por frases (sequências de palavras) é necessário um &lt;a href="https://juliocesarbatista.com/post/phrase-queries/" class="external-link" target="_blank" rel="noopener">índice &lt;em>k-gram&lt;/em>&lt;/a>.
Porém, criar um índice todas as combinações de termos pode ocupar muito espaço em disco/memória.
Principalmente se for necessário indexar combinações de 5 ou mais palavras, visto que muitas combinações podem aparecer apenas uma ou outra vez.
Uma solução para esse problema é um &lt;em>positional index&lt;/em> (índice de posições).&lt;/p>
&lt;p>Em um índice invertido, termos são mapeados para listas com &lt;em>ids&lt;/em> de documentos.
No &lt;em>positional index&lt;/em>, além dos &lt;em>ids&lt;/em>, também são mantidas as posições em que o termo aparece no documento.&lt;/p></description></item><item><title>Compressão de índices: Variable Byte Encoding</title><link>https://juliocesarbatista.com/posts/compressao-indices-vbe/</link><pubDate>Wed, 15 Jan 2020 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/compressao-indices-vbe/</guid><description>&lt;p>Uma vez que o índice invertido está montado com &lt;em>postings lists&lt;/em>, é necessário persistí-lo em disco.
O detalhe é que, se o índice for persistido como texto em UTF-8, cada caractére vai requisitar ao menos 8 bytes.
Portando, o id &lt;code>4568912&lt;/code> requer 7 bytes para ser armazenado.
A contrapartida é que, se for armazenado como um numérico (&lt;code>int&lt;/code> por exemplo), precisa de apenas 4 bytes.
Porém, é possível conseguir uma melhora na compressão ao considerar a estrutura de dados que será armazenada.
Nesse caso, para cada &lt;em>termo&lt;/em>, uma lista de números é persistida.
Por exemplo, o termo &lt;em>t&lt;/em> tem a lista de ids &lt;code>[652389, 652390, 652399, 652659]&lt;/code>, requisitando &lt;code>4 * 4 * 8 = 128 bytes&lt;/code>.
Mas é possível ir além dado que a lista de ids é ordenada, assim é possível guardar apenas os &lt;em>gaps&lt;/em> (saltos) entre os ids.
Seguindo o exemplo anterior, a lista a ser persistida seria &lt;code>[652389, 1, 9, 260]&lt;/code> porque &lt;code>[652389, 652390 = 652389 + 1, 652399 = 652390 + 9, 652659 = 652399 + 260]&lt;/code>.
Mesmo assim, asinda é necessário guardar 4 inteiros totalizando 128 bytes.
Mas agora, obervando bem os valores, é possível perceber que apenas &lt;code>652389&lt;/code> e &lt;code>260&lt;/code> requerem um tipo &lt;code>int&lt;/code>, os valores &lt;code>1&lt;/code> e &lt;code>9&lt;/code> podem ser armazenados com apenas 1 byte cada um (1 byte pode armazenar valores de 0 à 255).
Assim, o espaço foi reduzido para &lt;code>2 * 4 * 8 + 2 * 8 = 80 bytes&lt;/code> (um pouco mais da metade do espaço necessário anteriormente).&lt;/p></description></item><item><title>Executando consultas por frases</title><link>https://juliocesarbatista.com/posts/phrase-queries/</link><pubDate>Fri, 03 Jan 2020 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/phrase-queries/</guid><description>&lt;p>A partir de um &lt;a href="https://juliocesarbatista.com/post/indice-invertido/" class="external-link" target="_blank" rel="noopener">índice de termos/documentos&lt;/a> só é possível efetuar consultas de ocorrência de termos e filtros com operadores &lt;code>AND&lt;/code>, &lt;code>OR&lt;/code> e &lt;code>NOT&lt;/code>.
Entretanto, o que é preciso para executar uma consulta por &lt;em>presidente do Brasil&lt;/em>?
A forma mais simples, é converter essa consulta em &lt;code>presidente AND do AND Brasil&lt;/code> (o &lt;em>do&lt;/em> pode ser removido se quiser remover &lt;em>stop words&lt;/em>).
O detalhe é que essa consulta vai retornar qualquer documento que contenha &lt;em>presidente&lt;/em> e &lt;em>Brasil&lt;/em>, mas que não não fale necessariamento do &lt;em>presidente do Brasil&lt;/em>.
Um exemplo seria: &lt;em>O presidente do conselho está trabalhando para aumentar os empregos no Brasil&lt;/em>.
Esse documento é retornado pela consulta &lt;code>presidente AND do AND Brasil&lt;/code>, mas não tem nada a ver com &lt;em>presidente do Brasil&lt;/em>.
Portanto, é necessário melhorar a estrutura de índice para obter melhores resultados pelas consultas.&lt;/p></description></item><item><title>Algoritmos para consultar em índices</title><link>https://juliocesarbatista.com/posts/algoritmos-indices/</link><pubDate>Tue, 31 Dec 2019 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/algoritmos-indices/</guid><description>&lt;p>Uma das vantagens do &lt;a href="https://juliocesarbatista.com/post/indice-invertido/" class="external-link" target="_blank" rel="noopener">índice invertido&lt;/a> é a possibilidade de otimizar os algoritmos utilizados nas consultas.
Esses algoritmos já são implementados por &lt;code>set&lt;/code> no python, mas não garantem a sequência dos &lt;code>ids&lt;/code> dos documentos e também não permitem algumas otimizações.
Portanto, é necessário passar por esses algoritmos para ver as extensões e como elas podem ajudar.&lt;/p>
&lt;h2 id="intersecção-de-conjuntos">
 intersecção de conjuntos
 &lt;a class="heading-link" href="#intersec%c3%a7%c3%a3o-de-conjuntos">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link para o cabeçalho">&lt;/i>
 &lt;span class="sr-only">Link para o cabeçalho&lt;/span>
 &lt;/a>
&lt;/h2>
&lt;p>Uma consulta &lt;code>casa AND blumenau&lt;/code> precisa efetuar a intersecção entre os documentos de &lt;code>casa&lt;/code> e os documentos de &lt;code>blumenau&lt;/code>.
O algoritmo de intersecção é uma solução para esse problema&lt;/p></description></item><item><title>Considerações sobre a construção de índices</title><link>https://juliocesarbatista.com/posts/consideracoes-sobre-indices/</link><pubDate>Mon, 30 Dec 2019 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/consideracoes-sobre-indices/</guid><description>&lt;p>Para construir um índice, seja uma &lt;a href="https://juliocesarbatista.com/post/matriz-incidencia-termo-documento/" class="external-link" target="_blank" rel="noopener">matriz de incidência&lt;/a> ou um &lt;a href="https://juliocesarbatista.com/post/indice-invertido/" class="external-link" target="_blank" rel="noopener">índice invertido&lt;/a>, alguns detalhes em relação aos &lt;em>documentos&lt;/em> e aos &lt;em>termos&lt;/em> do vocabulário devem ser considerados.&lt;/p>
&lt;h2 id="documentos">
 Documentos
 &lt;a class="heading-link" href="#documentos">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link para o cabeçalho">&lt;/i>
 &lt;span class="sr-only">Link para o cabeçalho&lt;/span>
 &lt;/a>
&lt;/h2>
&lt;p>O primeiro ponto a se considerar é o que será indexado, o que será um &lt;em>documento&lt;/em> no índice.
Um livro pode ser considerado um &lt;em>documento&lt;/em>, o problema é que existem muitas palavras em um livro e diferentes palavras podem ocorrer em capítulos diferentes gerando um resultado não esperado.
Por exemplo, se o livro &lt;a href="https://www.amazon.com/Introduction-Algorithms-3rd-MIT-Press/dp/0262033844" class="external-link" target="_blank" rel="noopener">Introduction to Algorithms&lt;/a> for indexado, ele pode ser o resultado para a consulta &lt;code>graph AND quicksort&lt;/code>.
Não que o resultado esteja errado, essas palavras existem no livro, mas talvez o objetvo da consulta era encontrar uma forma de ordenar um grafo com &lt;em>quicksort&lt;/em>.
A solução é indexar partes menores do conteúdo, capítulos, páginas ou parágrafos.
Se levar esse concceito adiante, é possível indexar frases, porém o conteúdo será tão granular que muitas consultas podem falhar porque duas palavras não aparecem na mesma frase, mas no mesmo parágrafo.
Também é importante notar que a quantidade de documentos tem influência direta no tamanho do índice.
Portanto, será necessário um índice grande para armazenar muitos documento pequenos.&lt;/p></description></item><item><title>Índice invertido (inverted index)</title><link>https://juliocesarbatista.com/posts/indice-invertido/</link><pubDate>Sat, 28 Dec 2019 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/indice-invertido/</guid><description>&lt;p>&lt;a href="https://juliocesarbatista.com/post/matriz-incidencia-termo-documento/" class="external-link" target="_blank" rel="noopener">A matriz de incidência termo-documento&lt;/a> é uma das formas de representar um índice de termos por documento.
Mesmo usando o conceito de uma matriz esparsa, essa estrutura pode crescer muito para ser usada em memória.
Uma alternativa para esse caso é usar um índice invertido (&lt;em>inverted index&lt;/em>).&lt;/p>
&lt;p>Dados os seguintes documentos como exemplo:&lt;/p>
&lt;ul>
&lt;li>Uma casa à venda em Blumenau&lt;/li>
&lt;li>Vendo terreno em Gaspar&lt;/li>
&lt;li>Alugo apartamento em Indaial&lt;/li>
&lt;/ul>
&lt;p>A matriz de incidência é:&lt;/p></description></item><item><title>Matriz de incidência termo-documento</title><link>https://juliocesarbatista.com/posts/matriz-incidencia-termo-documento/</link><pubDate>Fri, 27 Dec 2019 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/matriz-incidencia-termo-documento/</guid><description>&lt;p>Para obter as ocorrências de uma &lt;em>query booleana&lt;/em>, por exemplo, &lt;code>casa AND blumenau&lt;/code> seria necessário passar em todos os documentos procurando por &lt;code>casa&lt;/code> e depois procurar novamente por &lt;code>blumenau&lt;/code>.
De certa forma, essa abordagem não é completamente ruim.
Mas existem algumas abordagens que podem melhorar o tempo da consulta e consumo de memória.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>Agora considere uma nova consulta, por exemplo, &lt;code>casa AND gaspar&lt;/code>, seria necessário repassar em todos os documentos novamente.
Assim, existe um certo custo computacional que pode ser otimizado.
Uma forma de evitar esse problema é criando um índice (assim como no final de um livro, onde são listadas as palavras e as páginas que elas aparecem).
Nesse caso, a consulta pode ser simplificada procurando pelas palavras &lt;code>casa&lt;/code> e &lt;code>blumenau&lt;/code> e filtrar apenas os documentos onde ambas aparecem.
Note que a otimização foi em relação à não precisar passar em todas as palavras de todos os documentos.&lt;/p></description></item><item><title>Soma dos números pares na sequência Fibonacci</title><link>https://juliocesarbatista.com/posts/soma-pares-fibonacci/</link><pubDate>Thu, 11 Jul 2019 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/soma-pares-fibonacci/</guid><description>&lt;p>Os números de Fibonacci
(&lt;a href="https://pt.wikipedia.org/wiki/Sequ%C3%AAncia_de_Fibonacci" class="external-link" target="_blank" rel="noopener">Wikipedia&lt;/a>,
&lt;a href="https://en.wikipedia.org/wiki/Fibonacci_number" class="external-link" target="_blank" rel="noopener">Wikipedia&lt;/a>)
são uma sequência definida como $F_n = F_{n-2} + F_{n-1}$.
Os primeiros números são &lt;em>0&lt;/em> e &lt;em>1&lt;/em>, e a partir deles, a sequência segue:
&lt;em>0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, &amp;hellip;&lt;/em>.
Dada a definição da sequência Fibonacci, como se calcula a soma dos números pares até $F_n &amp;lt; C$?&lt;/p>
&lt;p>A solução simples é fazer um &lt;em>script&lt;/em> conforme&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-fallback" data-lang="fallback">&lt;span style="display:flex;">&lt;span>C = 100
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>F_n2 = 0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>F_n1 = 1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>soma = 0
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>while True:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> F_n = F_n2 + F_n1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> if F_n &amp;gt;= C:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> break
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> elif F_n % 2 == 0:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> soma += F_n
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> F_n2 = F_n1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> F_n1 = F_n
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>print(soma)
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Mas é possível fazer melhor?
Sim, e isso requer pensar em como um número na sequência Fibonacci é calculado.&lt;/p></description></item><item><title>Experimentos com Machine Learning</title><link>https://juliocesarbatista.com/posts/experimentos-ml/</link><pubDate>Tue, 09 Jul 2019 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/experimentos-ml/</guid><description>&lt;p>O problema que &lt;em>machine learning&lt;/em> visa resolver é &lt;em>encontrar a melhor função de decisão dado um conjunto de dados&lt;/em>.
Portanto, este problem contém um conjunto de entrada $\mathcal{X}$, um algoritmo de aprendizado $\mathcal{A}$ e, normalmente, um conjunto de saídas esperadas $\mathcal{y}$.
Assim, como é possível saber se a função de decisão $\mathcal{f}$ gerada por $\mathcal{A}$ é realmente útil?
A resposta é que é necessário avaliar um experimento sobre o desempenho (&lt;em>performance&lt;/em>) de $\mathcal{f}$ em um conjunto de dados.
Se levar em consideração apenas os dados utilizados para gerar $\mathcal{f}$, muito provavelmente o desempenho será bom/ótimo.
Dessa forma é necessário encontrar um conjunto similar a $\mathcal{X}$ e $\mathcal{y}$ que possa ser usado para avaliação.
A forma mais simples de encontrar esse conjunto é separar uma parte de $\mathcal{X}$ e $\mathcal{y}$ para treino e outra para teste.
Existem algumas formas de fazer essa separação e cada uma varia de acordo com o problema, por exemplo:
&lt;em>leave one out&lt;/em>, &lt;em>k-fold cross validation&lt;/em>, &lt;em>forward chain&lt;/em> (para séries temporais) e outros.
O objetivo aqui não é discorrer sobre a melhor forma de separar os dados para o experimento, mas é levantar alguns pontos que devem ser considerados ao separar os dados para treino e teste.
Alguns problemas que devem ser evitados:&lt;/p></description></item><item><title>Técnicas de combinatória</title><link>https://juliocesarbatista.com/posts/combinatoria/</link><pubDate>Mon, 27 May 2019 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/combinatoria/</guid><description>&lt;p>Dado um conjunto $S$ qualquer com $n$ elementos,
de quantas formas é possível organizar/ordenar os elementos?
Quantos subconjuntos de $S$ com $k &amp;lt;= n$ elementos podem ser criados?
Essas respostas podem ser encontradas com algumas técnicas de combinatória.&lt;/p>
&lt;h2 id="definições">
 Definições
 &lt;a class="heading-link" href="#defini%c3%a7%c3%b5es">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link para o cabeçalho">&lt;/i>
 &lt;span class="sr-only">Link para o cabeçalho&lt;/span>
 &lt;/a>
&lt;/h2>
&lt;p>$ S = \text{ { A, B, C, D } } $ e $n = \lvert S \rvert = 4$&lt;/p>
&lt;p>&lt;strong>De quantas formas é possível organizar/ordenar os elementos?&lt;/strong>&lt;/p></description></item><item><title>Votes, scores e ranks</title><link>https://juliocesarbatista.com/posts/votes-scores-ranks/</link><pubDate>Fri, 10 May 2019 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/votes-scores-ranks/</guid><description>&lt;p>&lt;em>Votes&lt;/em>, &lt;em>scores&lt;/em> e &lt;em>ranks&lt;/em> são, normalmente, utilizados em conjunto.
Por exemplo, &lt;em>scores&lt;/em> são funções utilizadas para reduzir dados multi-dimensionais para uma única dimensão.
Com base nesse &lt;em>score&lt;/em>, é possível ordenar os dados através de um &lt;em>rank&lt;/em>.
Assim, é possível acessar o &lt;em>n&lt;/em>-ésimo elemento a partir de um &lt;em>rank&lt;/em>.
&lt;em>Votes&lt;/em>, é um dos exemplos que pode ser reduzido a partir de uma função &lt;em>score&lt;/em>.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>Um exemplo comum é o &lt;em>ranking&lt;/em> de produtos em uma página baseado na avaliação dos usuários.
Nesse caso, a avaliação dos usuários são os &lt;em>votes&lt;/em>.
A partir destes, um &lt;em>score&lt;/em> (normalmente a média,
&lt;a href="http://www.evanmiller.org/how-not-to-sort-by-average-rating.html" class="external-link" target="_blank" rel="noopener">que não é uma boa opção&lt;/a>)
é calculado.
Finalmente, os produtos são apresentados em um &lt;em>ranking&lt;/em> com base no &lt;em>score&lt;/em>.&lt;/p></description></item><item><title>Limpando dados (data cleaning)</title><link>https://juliocesarbatista.com/posts/data-cleaning/</link><pubDate>Sat, 20 Apr 2019 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/data-cleaning/</guid><description>&lt;p>A limpeza dos dados é um passo importante na construção de uma análise e modelo.
Entretanto, não existe um fluxo exato para seguir, a ideia é explorar o dataset e identificar registros inválidos e aplicar regras para corrigí-los.
A seguir tem uma lista do que procurar/corrigir em &lt;em>datasets&lt;/em>.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h1 id="1-conversão-de-tipos">
 1. Conversão de tipos
 &lt;a class="heading-link" href="#1-convers%c3%a3o-de-tipos">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link para o cabeçalho">&lt;/i>
 &lt;span class="sr-only">Link para o cabeçalho&lt;/span>
 &lt;/a>
&lt;/h1>
&lt;p>Simples, se o valor deve ser um numérico e está como &lt;em>string&lt;/em>, então usa-se o tipo mais apropriado. Normalmente &lt;code>float&lt;/code> para medidas contínuas e &lt;code>int&lt;/code> para números discretos.
O mesmo vale para &lt;code>bool&lt;/code>, datas e outros.
Também vale ressaltar a precisão de valores &lt;code>float&lt;/code>.
Se existem valores que precisam de cinco casas decimais e valores com duas casas decimais.
Arredondar todos os valores para duas casas decimais é uma boa opção.&lt;/p></description></item><item><title>Python: conjuntos (sets)</title><link>https://juliocesarbatista.com/posts/python-sets/</link><pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/python-sets/</guid><description>&lt;p>Vamos falar um pouco sobre &lt;em>sets&lt;/em> (conjuntos) em &lt;em>python&lt;/em>.
Se você já trabalha com a linguagem, provavelmente já se deparou com esses casos, mas se está iniciando na linguagem, talvez sejam exemplos interessantes para aprender.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;h2 id="por-que-conjuntos">
 Por que conjuntos?
 &lt;a class="heading-link" href="#por-que-conjuntos">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link para o cabeçalho">&lt;/i>
 &lt;span class="sr-only">Link para o cabeçalho&lt;/span>
 &lt;/a>
&lt;/h2>
&lt;p>Conjuntos são coleções que não permitem elementos duplicados. Dessa forma, são uma ótima estrutura de dados para verificar se um elemento já existe e garantir que esse elemento exista apenas uma vez.&lt;/p></description></item><item><title>Regressão múltipla com redes neurais convolucionais para estimativa conjunta da intensidade de Action Units</title><link>https://juliocesarbatista.com/posts/wtd-2017/</link><pubDate>Tue, 30 Oct 2018 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/wtd-2017/</guid><description>&lt;p>&lt;strong>Abstract&lt;/strong>: Este trabalho apresenta uma rede neural convolucional (CNN - Convolutional Neural Network) para efetuar a estimativa conjunta da intensidade de Action Units (AUs) em imagens de faces. A estimativa da intensidade de AUs é essencial durante a análise de expressões faciais. Os métodos existentes não levam em consideração a possibilidade de estimativa conjunta para vários AUs em uma face e precisam de um modelo para cada AU; métodos que fazem uso dessa informação precisam reestruturar o problema como aprendizado estruturado com grafos, aumentando a complexidade. Portanto, este trabalho propõe um modelo de regressão múltipla para realizar essa estimativa conjunta e permitir a otimização de um modelo end-to-end. O modelo proposto foi avaliado na base BP4D (Binghamton-Pittsburgh 3D Dynamic Spontaneous Facial Expression Database), utilizada no Facial Expression Recognition and Analysis Challenge (FERA) 2015, que possui anotações da intensidade para cinco AUs em imagens com ambiente controlado. Os resultados obtidos, na média das cinco AUs, superam os baselines propostos e são similares ao estado-da-arte, superando-o em uma das AUs.&lt;/p></description></item><item><title>Matemágica para devs</title><link>https://juliocesarbatista.com/posts/matemagica-para-devs/</link><pubDate>Mon, 13 Aug 2018 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/matemagica-para-devs/</guid><description>&lt;p>Oi, tudo certo!?&lt;/p>
&lt;p>Nos dias 28/07/2018 e 11/08/2018 eu apresentei duas palestras sobre matemática com foco em conceitos úteis para desenvolvedores de software.
O conteúdo está disponível no &lt;a href="https://github.com/ejulio/talks/tree/master/matemagica-para-devs" class="external-link" target="_blank" rel="noopener">GitHub&lt;/a>.
De forma geral, vimos:&lt;/p>
&lt;ul>
&lt;li>Conceitos fundamentais da álgebra;&lt;/li>
&lt;li>Distribuições e os conceitos de média, moda, mediana e desvio padrão na estatística;&lt;/li>
&lt;li>Matrizes, vetores e &lt;em>norms&lt;/em> na álgebra linear;&lt;/li>
&lt;li>Funções, derivadas e otimização com cálculo.&lt;/li>
&lt;/ul>
&lt;p>A ideia era aproximar esses conceitos com Python e bibliotecas que facilitam essas operações.&lt;/p></description></item><item><title>Sobre métodos ágeis</title><link>https://juliocesarbatista.com/posts/sobre-metodos-ageis/</link><pubDate>Sat, 10 Feb 2018 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/sobre-metodos-ageis/</guid><description>&lt;p>Vamos falar sobre metodologias ágeis, principalmente em um SAAS (&lt;em>Software as a Service&lt;/em>). Para começar, não sou um &lt;em>expert&lt;/em> em metodologias ágeis, assim como não passei por todos os possíveis casos que eles podem envolver. Portanto, posso ter uma visão enviesada para os conceitos que acredito fazer mais sentido. Depois desse &lt;em>disclaimer&lt;/em>, vamos lá!&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>Não me leve a mal, eu acho que Scrum (e outras metodologias baseadas em &lt;em>sprint&lt;/em>) é uma metodologia fantástica, só não acredito que ele seja o melhor processo para uma empresa que trabalha com um SAAS. Por exemplo, o primeiro princípio ágil diz, basicamente, que &lt;em>entregamos e validamos o software continua e rapidamente com o cliente&lt;/em>. Porém, se você está em um SAAS e trabalha com &lt;em>sprints&lt;/em> de duas semanas, as entregas só ocorrem no final das &lt;em>sprints&lt;/em>, ou seja, o cliente só vê a nova &lt;em>feature&lt;/em> à cada 15 dias. Bem, se considerar os casos mais antigos, no RUP, as entregas eram a cada x meses, então estamos bem.&lt;/p></description></item><item><title>Processamento de imagens com python</title><link>https://juliocesarbatista.com/posts/processamento-imagens-python-grupy/</link><pubDate>Sat, 20 Jan 2018 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/processamento-imagens-python-grupy/</guid><description>&lt;p>Olá, tudo bem!?&lt;/p>
&lt;p>Na última semana de janeiro, ainda não tem data definida, vou falar sobre processamento de imagens com python e &lt;a href="http://scikit-image.org/" class="external-link" target="_blank" rel="noopener">scikit-image&lt;/a> no &lt;a href="https://www.meetup.com/hackerspaceblumenau/events/trlwpnyxcbkc/" class="external-link" target="_blank" rel="noopener">Grupy Blumenau&lt;/a>.
O notebook com o conteúdo da apresentação já está &lt;a href="https://github.com/ejulio/talks/blob/master/processamento-imagens-python/Processamento%20de%20imagens%20com%20python.ipynb" class="external-link" target="_blank" rel="noopener">disponível no GitHub&lt;/a>.&lt;/p>
&lt;p>Aparece por lá para conferir e trocar uma ideia :D&lt;/p></description></item><item><title>Objectives and Key-Results (OKRs)</title><link>https://juliocesarbatista.com/posts/okrs/</link><pubDate>Tue, 09 Jan 2018 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/okrs/</guid><description>&lt;p>Vamos falar sobre desenvolvimento de software.&lt;/p>
&lt;p>&lt;em>Objectives and Key-Results&lt;/em> (OKRs) são uma prática/&lt;em>framework&lt;/em>/metodologia para definir objetivos, os &lt;em>Objectives&lt;/em>, e definir resultados que indiquem o quão próximos estamos desses objetivos, os &lt;em>Key-Results&lt;/em>.
Essa prática não é um &lt;em>framework&lt;/em> ágil como o Scrum/XP/Kanban, mas sim uma forma de alinhar os objetivos da empresa com todos os indivíduos e como cada um pode contribuir nesses objetivos.
Com isso, é possível colocar o &lt;em>template&lt;/em> padrão de OKRs:&lt;/p></description></item><item><title>Como funcionam as redes neurais</title><link>https://juliocesarbatista.com/posts/como-funcionam-as-redes-neurais/</link><pubDate>Thu, 30 Nov 2017 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/como-funcionam-as-redes-neurais/</guid><description>&lt;p>Olá, tudo bem!?&lt;/p>
&lt;p>Amanhã, 30 de novembro, eu vou apresentar a palestra &amp;ldquo;Como funcionam as rdes neurais&amp;rdquo; no &lt;a href="https://www.meetup.com/GDG-Blumenau/" class="external-link" target="_blank" rel="noopener">GDG Blumenau&lt;/a> aqui em Blumenau, Santa Catarina. Os slides e exemplo da palestra estão disponíveis &lt;a href="https://github.com/ejulio/talks/tree/master/como-funcionam-as-redes-neurais" class="external-link" target="_blank" rel="noopener">aqui&lt;/a>.&lt;/p>
&lt;p>Aparece por lá para conferir e trocar uma ideia :D&lt;/p></description></item><item><title>Face Analysis in the Wild</title><link>https://juliocesarbatista.com/posts/face-analysis-in-the-wild/</link><pubDate>Tue, 17 Oct 2017 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/face-analysis-in-the-wild/</guid><description>&lt;p>&lt;strong>Abstract&lt;/strong>: With the global demand for extra security systems, and the growing of human-machine interaction, facial analysis in unconstrained environments (in the wild) became a hot-topic in recent computer vision research. Unconstrained environments include surveillance footage, social media photos and live broadcasts. This type of images and videos include no control over illumination, position, size, occlusion, and facial expressions. Successful facial processing methods for controlled scenarios are unable to pledge with challenging circumstances. Consequently, methods tailored for handling those situations are indispensable for the face analysis research progress. This work presents a comprehensive review of state-of-the-art methods, drawing attention to the complications derived from in the wild scenariosa nd the behavior differences when applied to the controlled images. The main topics to be covered are: (1) face detection; (2) facial image quality; (3) head pose estimation; (4) face alignment; (5) 3D face reconstruction; (6) gender and age estimation; (7) facial expressions and emotions; and (8) face recognition. Finally, available code and applications for in the wild face analysis are presented, followed by a discussion on future directions.&lt;/p></description></item><item><title>A normalização e o Gradient Descent</title><link>https://juliocesarbatista.com/posts/a-normalizacao-e-o-gradient-descent/</link><pubDate>Wed, 27 Sep 2017 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/a-normalizacao-e-o-gradient-descent/</guid><description>&lt;p>Vamos falar sobre machine learning.
Estou [enfim] participando do &lt;em>&lt;a href="https://www.coursera.org/learn/machine-learning" class="external-link" target="_blank" rel="noopener">MOOC&lt;/a>&lt;/em> do &lt;a href="https://twitter.com/AndrewYNg" class="external-link" target="_blank" rel="noopener">Andrew Ng, Ph. D.&lt;/a> no &lt;a href="http://coursera.org" class="external-link" target="_blank" rel="noopener">Coursera&lt;/a> e me deparei com a importância de normalizar os dados antes de efetuar a otimização do algoritmo de aprendizado.
Não leve a mal, sei, há certo tempo, que é importante normalizar os valores de entrada para que o algoritmo tenha uma melhor, e mais rápida, convergência.
Entretanto, nunca havia, ao menos até onde percebi, me deparado com o quanto esse pré-processamento implica no processo.&lt;/p></description></item><item><title>Conheça o machine learning: algoritmos que aprendem a partir de dados, imagens e texto</title><link>https://juliocesarbatista.com/posts/conheca_o_machine_learning/</link><pubDate>Sun, 13 Aug 2017 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/conheca_o_machine_learning/</guid><description>&lt;p>Olá, tudo bem!?&lt;/p>
&lt;p>Amanhã, 14 de agosto, eu vou apresentar a palestra &amp;ldquo;Conheça o machine learning: algoritmos que aprendem a partir de dados, imagens e texto&amp;rdquo; na &lt;a href="http://www.fai.com.br/portal/semanainfo/index.php" class="external-link" target="_blank" rel="noopener">XXI Semana de Informática da UNIFAI&lt;/a> em Adamantina, São Paulo. Os slides da palestra podem ser visualizados &lt;a href="https://github.com/ejulio/talks/blob/master/conheca_o_machine_learning/conheca_o_machine_learning.pdf" class="external-link" target="_blank" rel="noopener">aqui&lt;/a>.&lt;/p>
&lt;p>Espero que você goste!&lt;/p></description></item><item><title>AUMPNet: Simultaneous Action Units Detection and Intensity Estimation on Multipose Facial Images Using a Single Convolutional Neural Network</title><link>https://juliocesarbatista.com/posts/aumpnet/</link><pubDate>Sat, 03 Jun 2017 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/aumpnet/</guid><description>&lt;p>&lt;strong>Abstract&lt;/strong>: This paper presents an unified convolutional neural network (CNN), named AUMPNet, to perform both Action Units (AUs) detection and intensity estimation on facial images with multiple poses. Although there are a variety of methods in the literature designed for facial expression analysis, only few of them can handle head pose variations. Therefore, it is essential to develop new models to work on non-frontal face images, for instance, those obtained from unconstrained environments. In order to cope with problems raised by pose variations, an unique CNN, based on region and multitask learning, is proposed for both AU detection and intensity estimation tasks. Also, the available head pose information was added to the multitask loss as a constraint to the network optimization, pushing the network towards learning better representations. As opposed to current approaches that require ad hoc models for every single AU in each task, the proposed network simultaneously learns AU occurrence and intensity levels for all AUs. The AUMPNet was evaluated on an extended version of the BP4D-Spontaneous database, which was synthesized into nine different head poses and made available to FG 2017 Facial Expression Recognition and Analysis Challenge (FERA 2017) participants. The achieved results surpass the FERA 2017 baseline, using the challenge metrics, for AU detection by 0.054 in F1-score and 0.182 in ICC(3, 1) for intensity estimation.&lt;/p></description></item><item><title>Desmistificando o Deep Learning com TensorFlow</title><link>https://juliocesarbatista.com/posts/desmistificando-o-deep-learning-com-tensorflow/</link><pubDate>Wed, 03 May 2017 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/desmistificando-o-deep-learning-com-tensorflow/</guid><description>&lt;p>Olá, tudo bem!?&lt;/p>
&lt;p>Eu e o &lt;a href="https://twitter.com/vitoralbiero" class="external-link" target="_blank" rel="noopener">Vítor Albiero&lt;/a> vamos falar sobre deep learning na &lt;a href="http://www.thedevelopersconference.com.br/tdc/2017/florianopolis/trilha-datascience-e-machine-learning" class="external-link" target="_blank" rel="noopener">The Developers Conference (TDC) Florianópolis 2017 na trilha de Data Science e Machine Learning&lt;/a>. A palestra se chama &amp;ldquo;Desmistificando o Deep Learning com TesnsorFlow&amp;rdquo; e também vai estar na trilha stadium da Intel e vai ser &lt;a href="https://www.eventials.com/Globalcode/quinta-de-manha-stadium-intel-floripa-2017/" class="external-link" target="_blank" rel="noopener">transmitida gratuitamente pela internet&lt;/a>.&lt;/p>
&lt;p>Nesses links você pode acessar os &lt;a href="https://github.com/ejulio/talks/blob/master/desmistificando-o-deep-learning-com-tensorflow/Desmistificando_o_Deep_Learning_com_TensorFlow.pdf" class="external-link" target="_blank" rel="noopener">slides&lt;/a> da talk e também os &lt;a href="https://github.com/ejulio/talks/blob/master/desmistificando-o-deep-learning-com-tensorflow" class="external-link" target="_blank" rel="noopener">códigos&lt;/a>.&lt;/p>
&lt;p>Espero que você goste!&lt;/p></description></item><item><title>Landmark-free smile intensity estimation</title><link>https://juliocesarbatista.com/posts/landmark-free-smile-intensity-estimation/</link><pubDate>Fri, 01 Jul 2016 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/landmark-free-smile-intensity-estimation/</guid><description>&lt;p>&lt;strong>Abstract&lt;/strong>: Facial expression analysis is an important field of research, mostly because of the rich information faces can provide. The majority of works published in the literature have focused on facial expression recognition and so far estimating facial expression intensities have not gathered same attention. The analysis of these intensities could improve face processing applications on distinct areas, such as computer assisted health care, human-computer interaction and biometrics. Because the smile is the most common expression, studying its intensity is a first step towards estimating other expressions intensities. Most related works are based on facial landmarks, sometimes combined with appearance features around these points, to estimate smile intensities. Relying on landmarks can lead to wrong estimations due to errors in the registration step. In this work we investigate a landmark-free approach for smile intensity estimation using appearance features from a grid division of the face. We tested our approach on two different databases, one with spontaneous expressions (BP4D) and the other with posed expressions (BU-3DFE); results are compared to state-of-the-art works in the field. Our method shows competitive results even using only appearance features on spontaneous facial expression intensities, but we found that there is still need for further investigation on posed expressions.&lt;/p></description></item><item><title>Batman é você? Desenhando o símbolo do Batman com retas, parábolas e círculos</title><link>https://juliocesarbatista.com/posts/batman-e-voce-desenhando-o-simbolo-do-batman-com-retas-parabolas-e-circulos/</link><pubDate>Wed, 08 Jun 2016 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/batman-e-voce-desenhando-o-simbolo-do-batman-com-retas-parabolas-e-circulos/</guid><description>&lt;p>Olá, tudo certo!?&lt;/p>
&lt;p>Hoje vamos trabalhar com a ideia de desenhar o símbolo do &lt;em>Batman&lt;/em> com equações. Veremos alguns conceitos matemáticos que permitem fazer esse desenho. O desenho que eu vou demonstrar é um que fiz e talvez não seja com as melhores fórmulas nem a forma otimizada, mas acho que ele é bom um passo a passo sobre alguns conceitos da álgebra. O desenho é composto por retas, parábolas e círculos. A partir destes componentes básicos, aplicamos transformações e restrições no domínio da equação que formam o desenho. Para fazer o desenho vamos usar o &lt;a href="https://www.desmos.com/calculator" class="external-link" target="_blank" rel="noopener">Desmos&lt;/a> (calculadora gráfica online).&lt;/p></description></item><item><title>Cross-validation: testando o desempenho de um classificador</title><link>https://juliocesarbatista.com/posts/cross-validation-testando-o-desempenho-de-um-classificador/</link><pubDate>Fri, 27 May 2016 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/cross-validation-testando-o-desempenho-de-um-classificador/</guid><description>&lt;p>Olá pessoal, tudo certo!?&lt;/p>
&lt;p>Hoje vamos falar sobre aprendizado de máquina. Não vamos falar sobre as técnicas de classificação, mas sobre as técnicas de verificação de desempenho dos algoritmos.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;h1 id="dados-e-características">
 Dados e características
 &lt;a class="heading-link" href="#dados-e-caracter%c3%adsticas">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link para o cabeçalho">&lt;/i>
 &lt;span class="sr-only">Link para o cabeçalho&lt;/span>
 &lt;/a>
&lt;/h1>
&lt;p>O exemplo de teste será a classificação de texto baseado no &lt;a href="http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html" class="external-link" target="_blank" rel="noopener">tutorial de classificação de texto do &lt;em>scikit-learn&lt;/em>&lt;/a>. O código inicial é:&lt;/p></description></item><item><title>The Dolly Zoom Effect</title><link>https://juliocesarbatista.com/posts/the-dolly-zoom-effect/</link><pubDate>Thu, 27 Aug 2015 00:00:00 +0000</pubDate><guid>https://juliocesarbatista.com/posts/the-dolly-zoom-effect/</guid><description>&lt;h3 id="a-ideia">
 A ideia
 &lt;a class="heading-link" href="#a-ideia">
 &lt;i class="fa-solid fa-link" aria-hidden="true" title="Link para o cabeçalho">&lt;/i>
 &lt;span class="sr-only">Link para o cabeçalho&lt;/span>
 &lt;/a>
&lt;/h3>
&lt;p>No post de hoje eu gostaria de demonstrar como fazer o efeito 3D &lt;a href="http://codepen.io/ejulio/full/RPqgxp/" class="external-link" target="_blank" rel="noopener">&lt;em>dolly zoom effect&lt;/em>&lt;/a> utilizando a three.js.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;p>Antes de irmos para a parte do código, eu vou deixar o link do vídeo onde eu vi o efeito. O vídeo também fala de outros efeitos e eu vou deixar marcado em dois pontos importantes para explicar o efeito posteriormente.


 
 &lt;div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
 &lt;iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/Y2gTSjoEExc?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"
 >&lt;/iframe>
 &lt;/div>
&lt;/p></description></item></channel></rss>