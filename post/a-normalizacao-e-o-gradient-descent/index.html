<!doctype html><html class=no-js lang=en-us>
<head>
<meta charset=utf-8>
<title>A normalização e o Gradient Descent | Júlio César Batista</title>
<meta http-equiv=x-ua-compatible content="ie=edge">
<meta name=viewport content="width=device-width,initial-scale=1">
<link rel=stylesheet href=https://juliocesarbatista.com/css/foundation.min.css>
<link rel=stylesheet href=https://juliocesarbatista.com/css/highlight.min.css>
<link rel=stylesheet href=https://juliocesarbatista.com/css/font-awesome.min.css>
<link rel=stylesheet href=https://juliocesarbatista.com/css/academicons.min.css>
<link rel=stylesheet href=https://juliocesarbatista.com/css/fonts.css>
<link rel=stylesheet href=https://juliocesarbatista.com/css/finite.css>
</head>
<body>
<header>
<nav class=nav-bar>
<div class=title-bar data-responsive-toggle=site-menu data-hide-for=medium>
<button class=site-hamburger type=button data-toggle>
<i class="fa fa-bars fa-lg" aria-hidden=true></i>
</button>
<div class="title-bar-title site-title">
<a href=https://juliocesarbatista.com/>Júlio César Batista</a>
</div>
<div class="title-bar-right pull-right">
</div>
</div>
<div class=top-bar id=site-menu>
<div class="top-bar-title show-for-medium site-title">
<a href=https://juliocesarbatista.com/>Júlio César Batista</a>
</div>
<div class=top-bar-left>
<ul class="menu vertical medium-horizontal">
<li><a href=https://juliocesarbatista.com/publication/>Publicações</a></li>
<li><a href=https://juliocesarbatista.com/post/>Blog</a></li>
<li><a href=https://juliocesarbatista.com/logbook/>Recursos</a></li>
</ul>
</div>
<div class="top-bar-right show-for-medium">
</div>
</div>
</nav>
</header>
<main>
<div class=row>
<div class="column small-12 medium-10 medium-offset-1 end large-8 large-offset-0">
<article class=article itemscope itemtype=http://schema.org/Article>
<h1 itemprop=name>A normalização e o Gradient Descent</h1>
<div class=post-metadata>
<span class=post-date>
<time datetime="2017-09-27 00:00:00 +0000 UTC" itemprop=datePublished>
2017/09/27
</time>
</span>
<span class=post-categories>
<span class=nowrap>
<a href=https://juliocesarbatista.com/categories/aprendizado-de-m%c3%a1quina>Aprendizado de Máquina</a>
</span>
</span>
<span class=post-tags>
<span class=nowrap><i class="fa fa-tags"></i>
<a href=https://juliocesarbatista.com/tags/exemplos>Exemplos</a>
,
<a href=https://juliocesarbatista.com/tags/python>python</a>
,
<a href=https://juliocesarbatista.com/tags/normaliza%c3%a7%c3%a3o-de-caracter%c5%9biticas>Normalização de caracterśiticas</a>
,
<a href=https://juliocesarbatista.com/tags/gradient-descent>Gradient descent</a>
,
<a href=https://juliocesarbatista.com/tags/otimiza%c3%a7%c3%a3o>Otimização</a>
</span>
</span>
</div>
<div class=post-body itemprop=articleBody>
<p>Vamos falar sobre machine learning.
Estou [enfim] participando do <em><a href=https://www.coursera.org/learn/machine-learning>MOOC</a></em> do <a href=https://twitter.com/AndrewYNg>Andrew Ng, Ph. D.</a> no <a href=http://coursera.org>Coursera</a> e me deparei com a importância de normalizar os dados antes de efetuar a otimização do algoritmo de aprendizado.
Não leve a mal, sei, há certo tempo, que é importante normalizar os valores de entrada para que o algoritmo tenha uma melhor, e mais rápida, convergência.
Entretanto, nunca havia, ao menos até onde percebi, me deparado com o quanto esse pré-processamento implica no processo.</p>
<p>Se você já leu ou fez algum exemplo/tutorial sobre <em>machine learning</em>, provavelmente, sabe que é importante normalizar as entradas/características antes de iniciar o processo de otimização.
Geralmente, isso é feito com <em>standardizing</em> ao aplicar $\frac{\mathbf{x}_j - \mathbf{\mu}_j}{\mathbf{\sigma}_j}$ que vai deixar as características ($\mathbf{x}$) com média ($\mathbf{\mu}$) 0 e desvido padrão ($\mathbf{\sigma}$) 1.
Especialmente no caso do <em>gradient descent</em> (método de otimização baseado em derivadas), essa normalização é importante para &ldquo;garantir uma superfície mais uniforme&rdquo; de forma que o algoritmo consegue caminhar em direção ao melhor ponto da otimização.
Nesse caso, estava <a href=https://github.com/ejulio/coursera-machine-learning/blob/master/Week%203/Assignment%20Logistic%20Regression.ipynb>implementando</a> o <em>gradient descent</em> para otimizar uma simples regressão logísitica com 2 variáveis de entradas, um problema razoavelmente simples.
Porém, quando executei a rotina com ~1,000 iterações ao algoritmo mal e mal saiu do lugar.
Passei um tempo revirando o código, achei uns possíveis bugs e arrumei algumas coisas.
Mesmo assim, nada de otimizar, ou chegar próximo do valor esperado no exercício.
Com isso, fui verificar o fórum para ver se alguém teve o mesmo problema e me deparei com um usuário que precisou de 1,000,000 de iterações para convergir implementando em R.
Bem, fiz o teste e, realmente, com 1,000,000 de iterações minha implementação em python (não quero usar o MATLAB :b) convergiu para um valor aproximado ao informado no exercício.
Enfim, não fazia muito sentido 1,000,000 de iterações para um problema de classificação binária com 2 variáveis.
Depois de pesquisar mais um pouco e ver em algum lugar do fórum a palavra mágica <strong>normalização</strong>, me lembrei que não havia normalizado as características antes de iniciar a otimização.
Dito e feito, implementei a normalização e com um teste com ~10,000 iterações a minha implementação convergiu para o valor esperado no exercício.</p>
<p>Lição aprendida! Apesar de saber da importância de normalizar as entradas do algoritmo, nunca tinha pensado sobre o quão difícil é o problema sem normalizar.
Agora, tive um ótimo exemplo do que realmente está acontecendo durante esse processo de otimização.
Enfim, o que escrevi aqui não deve ser novidade para quem já fez algo com <em>machine learning</em>, mas pode ser que ajude a quem está começando a ter uma noção do porque de alguns detalhes serem importantes para o processo como um todo.</p>
</div>
<meta itemprop=wordCount content="447">
<meta itemprop=datePublished content="2017-09-27">
<meta itemprop=url content="https://juliocesarbatista.com/post/a-normalizacao-e-o-gradient-descent/">
</article>
<ul class=pagination role=navigation aria-label=Pagination>
<li class=arrow aria-disabled=true><a href=https://juliocesarbatista.com/post/conheca_o_machine_learning/>&#171; <em>Previous<span class=show-for-sr> page</span></em>: Conheça o machine learning: algoritmos que aprendem a partir de dados, imagens e texto</a></li>
<li class=arrow aria-disabled=true><a href=https://juliocesarbatista.com/post/como-funcionam-as-redes-neurais/><em>Next<span class=show-for-sr> page</span></em>: Como funcionam as redes neurais&nbsp;&#187;</a></li>
</ul>
</div>
</div>
</main>
<footer>
<hr>
<div class=row>
<div class="column small-10 small-centered text-center">
<em>Gostou ou tem algum comentário? Vamos conversar no <span class=nowrap><i class="fa fa-twitter-square"></i> Twitter</span>, <a href=http://twitter.com/ejuliobatista>@ejuliobatista</a>!</em>
</div>
</div>
</footer>
<div class=endofpage>
</div>
<script src=https://juliocesarbatista.com/js/jquery.js></script>
<script src=https://juliocesarbatista.com/js/what-input.js></script>
<script src=https://juliocesarbatista.com/js/foundation.min.js></script>
<script src=https://juliocesarbatista.com/js/finite.js></script>
<script src=https://juliocesarbatista.com/js/highlight.pack.js></script>
<script>hljs.initHighlightingOnLoad()</script>
<script type=text/x-mathjax-config>
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [['$','$'], ['\\(','\\)']],
          processEscapes: true
        }
      });
    </script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML" async></script>
</body>
</html>